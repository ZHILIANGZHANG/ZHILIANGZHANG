<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cheng Zhang (Rylan Zhang) - Academic Profile</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif:wght@400;500;600;700&family=Open+Sans:wght@300;400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #2c3e50;
            --background-color: #f5f5f5;
            --accent-color: #7f1734;
            --text-color: #333333;
            --text-secondary: #666666;
            --border-color: #e0e0e0;
            --container-width: 800px;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Open Sans', sans-serif;
            background-color: var(--background-color);
            color: var(--text-color);
            line-height: 1.6;
            padding: 2rem 1rem;
        }
        
        .container {
            max-width: var(--container-width);
            margin: 0 auto;
            background-color: white;
            padding: 3rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        
        header {
            margin-bottom: 2rem;
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 1.5rem;
        }
        
        h1 {
            font-family: 'Noto Serif', serif;
            color: var(--primary-color);
            font-size: 2rem;
            margin-bottom: 0.5rem;
            font-weight: 600;
        }
        
        .contact {
            color: var(--text-secondary);
            font-size: 1rem;
            margin-bottom: 1rem;
        }
        
        section {
            margin-bottom: 2.5rem;
        }
        
        h2 {
            font-family: 'Noto Serif', serif;
            color: var(--accent-color);
            font-size: 1.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
            font-weight: 500;
        }
        
        p {
            margin-bottom: 1rem;
            text-align: justify;
        }
        
        .education-details {
            margin-bottom: 1rem;
        }
        
        .education-details p {
            margin-bottom: 0.5rem;
        }
        
        .research-interests p {
            margin-bottom: 0.5rem;
        }
        
        .publication {
            margin-bottom: 1.5rem;
            padding-left: 1rem;
            border-left: 3px solid var(--border-color);
        }
        
        .publication-status {
            font-style: italic;
            color: var(--text-secondary);
            margin-bottom: 0.5rem;
        }
        
        .publication-title {
            font-weight: 600;
            margin-bottom: 0.5rem;
        }
        
        .publication-authors {
            margin-bottom: 0.5rem;
        }
        
        .publication-venue {
            color: var(--text-secondary);
        }
        
        .research-experience {
            margin-bottom: 1.5rem;
        }
        
        .research-experience-title {
            font-weight: 600;
            margin-bottom: 0.5rem;
        }
        
        .research-experience-details {
            margin-bottom: 0.5rem;
        }
        
        .award {
            margin-bottom: 1rem;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 2rem 1.5rem;
            }
            
            h1 {
                font-size: 1.8rem;
            }
            
            h2 {
                font-size: 1.3rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Cheng Zhang (Rylan Zhang)</h1>
            <div class="contact">zhangcheng2122@jlu.edu.cn</div>
        </header>
        
        <section>
            <h2>Objective</h2>
            <p>To pursue a Master's degree in Electrical Engineering at KAIST (Fall 2026), with a strong focus on advancing research in Multi-Modal Machine Learning, Computer Vision, and related cutting-edge AI fields. I am driven by a passion for developing innovative solutions that effectively bridge diverse data modalities and am eager to contribute to groundbreaking research with tangible real-world applications.</p>
        </section>
        
        <section>
            <h2>Education</h2>
            <div class="education-details">
                <p><strong>BSc, Computer Science & Technology</strong> | Jilin University | September 2022 – June 2026</p>
                <p>Average Score: 87.5/100</p>
                <p>GPA: 3.52/4.0</p>
                <p>Rank: 11/102</p>
            </div>
        </section>
        
        <section>
            <h2>Research Interests</h2>
            <div class="research-interests">
                <p>My research focuses on advancing the field of artificial intelligence through several interconnected areas. I am particularly interested in Multi-Modal Large Language Models, exploring how these systems can effectively integrate and process information across different modalities to enhance understanding and generation capabilities. My work in Computer Vision examines how machines perceive and interpret visual information, developing algorithms that can accurately recognize patterns and objects in complex environments. I also investigate Image Generation techniques, working to create more realistic and controllable visual outputs from textual descriptions or other input modalities. Additionally, my research in Affective Computing explores how AI systems can recognize, interpret, and respond to human emotions, bridging the gap between computational and emotional intelligence.</p>
            </div>
        </section>
        
        <section>
            <h2>Publications</h2>
            
            <div class="publication">
                <div class="publication-status">Preparing for Submission, 2025</div>
                <div class="publication-title">EmoArt: A Large-Scale, Fine-Grained Dataset of 56 Art Painting Categories Across Global Cultures for Emotion-Aware Analysis</div>
                <div class="publication-authors">Cheng Zhang, HongXia Xie, Bin Wen, Wen-Huang Cheng.</div>
                <div class="publication-venue">ACM Multimedia (Dataset Track) 2025.</div>
            </div>
            
            <div class="publication">
                <div class="publication-status">Under Review, 2025</div>
                <div class="publication-title">EmoDiscover: Vocabulary-free Emotion Discovery via Multi-modal Large Language Model</div>
                <div class="publication-authors">Hung-Jen Chen, Hongxia Xie, Cheng Zhang, Songhan Zuo, Chu-Jun Peng, Hong-Han Shuai, Yong Man Ro, Wen-Huang Cheng.</div>
                <div class="publication-venue">International Conference on Computer Vision (ICCV) 2025.</div>
            </div>
            
            <div class="publication">
                <div class="publication-status">Under Review, 2025</div>
                <div class="publication-title">Single Document Image Highlight Removal via A Large-Scale Real-World Dataset and A Location-Aware Network</div>
                <div class="publication-authors">Lu Pan, Yu-Hsuan Huang, Hongxia Xie, Cheng Zhang, Hongwei Zhao, Hong-Han Shuai, Wen-Huang Cheng.</div>
                <div class="publication-venue">ACM Multimedia (ACM MM) 2025.</div>
            </div>
            
            <div class="publication">
                <div class="publication-status">Preparing for Submission, 2025</div>
                <div class="publication-title">AmorisBench: A Comprehensive Multi-modal Dataset for Love-centric Question Answering</div>
                <div class="publication-authors">Chu-Jun Peng, Hongxia Xie, Cheng Zhang, Wen-Huang Cheng.</div>
                <div class="publication-venue">ACM Multimedia (Dataset Track) 2025.</div>
            </div>
        </section>
        
        <section>
            <h2>Research Experience</h2>
            
            <div class="research-experience">
                <div class="research-experience-title">Affective Vision Computing (AVC) Lab, Jilin University</div>
                <div class="research-experience-details">Research Assistant | 2024 – Present</div>
                <div class="research-experience-details">Supervisor: Associate Professor Hongxia Xie</div>
                <p>As a core member of the Affective Vision Computing Lab, I contribute to cutting-edge projects in affective computing and multi-modal machine learning. My work involves developing innovative approaches to understand and model emotional responses in visual and textual data, collaborating with an interdisciplinary team of researchers to advance the field of emotion-aware artificial intelligence.</p>
            </div>
            
            <div class="research-experience">
                <div class="research-experience-title">National Natural Science Foundation of China, Youth Project</div>
                <div class="research-experience-details">Research on Emotion Hallucination Elimination Technology for Visual-Language Pre-training Models</div>
                <div class="research-experience-details">Core Participant | January 2025 – Present</div>
                <p>I am actively involved in this foundational research project focused on addressing emotion hallucination issues in visual-language pre-training models. My contributions include developing methodologies to identify and mitigate instances where models incorrectly attribute emotional content that is not present in the source material, thereby improving the reliability and accuracy of emotion recognition in multimodal systems.</p>
            </div>
            
            <div class="research-experience">
                <div class="research-experience-title">Jilin Provincial Department of Education, Excellent Youth Project</div>
                <div class="research-experience-details">Research on Emotion Understanding Stability Technology for Multimodal Large Models</div>
                <div class="research-experience-details">Core Participant | January 2025 – Present</div>
                <p>In this provincial-level research initiative, I work on enhancing the stability and consistency of emotion understanding capabilities in multimodal large models. The project aims to develop robust techniques that ensure reliable emotional interpretation across diverse contexts and input variations, contributing to more dependable affective computing applications.</p>
            </div>
        </section>
        
        <section>
            <h2>Honors & Awards</h2>
            
            <div class="award">
                <p><strong>Undergraduate Scholarship</strong>, Jilin University, 2023-2024</p>
            </div>
            
            <div class="award">
                <p><strong>Undergraduate Scholarship</strong>, Jilin University, 2022-2023</p>
            </div>
        </section>
    </div>
</body>
</html>
